import cmd
import os
import sys
from pathlib import Path
from typing import Optional
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class IngestionCLI(cmd.Cmd):
    """ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• ì¸í„°ë™í‹°ë¸Œ CLI"""
    
    intro = """

Commands:
  - run     : ì‘ì—… ì‹¤í–‰
  - status  : í˜„ì¬ ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
  - help    : ë„ì›€ë§
  - exit    : ì¢…ë£Œ

"""
    prompt = ' ingestion> '
    
    def __init__(self):
        super().__init__()
        # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
        self.base_path = Path("/app/data")
        self.source_path = self.base_path / "source_documents"
        self.extracted_path = self.base_path / "extracted_documents"
        self.chunked_path = self.base_path / "chunked_documents"
        self.index_path = Path("/app/search_indexes")
    
    def do_run(self, arg):
        """
        ì‘ì—… ì‹¤í–‰
        
        ì‚¬ìš©ë²•:
          run --mode <mode> --file <filename>
          run -m <mode> -f <filename>
          
        ì˜ˆì‹œ:
          run --mode parsing --file create_std_contract.pdf
          run -m parsing -f create_std_contract.pdf
          run -m full -f all
          run --mode chunking --file create_std_contract.json
          run -m embedding -f create_std_contract_chunks.jsonl
        
        ëª¨ë“œ:
          - full      : ì „ì²´ íŒŒì´í”„ë¼ì¸ (íŒŒì‹±â†’ì²­í‚¹â†’ì„ë² ë”©â†’ì¸ë±ì‹±)
          - parsing   : PDF íŒŒì‹±ë§Œ
          - chunking  : JSON ì²­í‚¹ë§Œ
          - embedding : ì„ë² ë”© + ì¸ë±ì‹±
        
        íŒŒì¼:
          - all             : ëª¨ë“  íŒŒì¼
          - <filename>      : íŠ¹ì • íŒŒì¼ í•˜ë‚˜
        
        ì°¸ê³ :
          - íŒŒì¼ëª…ì— 'guidebook' í¬í•¨ â†’ í™œìš©ì•ˆë‚´ì„œ ëª¨ë“ˆ ì‚¬ìš©
          - ê·¸ ì™¸ íŒŒì¼ â†’ í‘œì¤€ê³„ì•½ì„œ ëª¨ë“ˆ ì‚¬ìš©
        """
        try:
            # ì¸ì íŒŒì‹±
            args = self._parse_run_args(arg)
            if not args:
                return
            
            mode = args.get('mode')
            filename = args.get('file')
            
            logger.info("=" * 60)
            logger.info(f" ì‘ì—… ì‹œì‘")
            logger.info(f"  ëª¨ë“œ: {mode}")
            logger.info(f"  íŒŒì¼: {filename}")
            logger.info("=" * 60)
            
            # ëª¨ë“œë³„ ì‹¤í–‰
            if mode == 'full':
                self._run_full_pipeline(filename)
            elif mode == 'parsing':
                self._run_parsing(filename)
            elif mode == 'chunking':
                self._run_chunking(filename)
            elif mode == 'embedding':
                self._run_embedding(filename)
            else:
                logger.error(f" ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {mode}")
                return
            
            logger.info("=" * 60)
            logger.info(" ì‘ì—… ì™„ë£Œ")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f" ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    def _parse_run_args(self, arg):
        """run ëª…ë ¹ì–´ ì¸ì íŒŒì‹±"""
        args = {}
        tokens = arg.split()
        
        i = 0
        while i < len(tokens):
            if tokens[i] in ['--mode', '-m'] and i + 1 < len(tokens):
                mode = tokens[i + 1]
                if mode not in ['full', 'parsing', 'chunking', 'embedding']:
                    logger.error(f" ì˜ëª»ëœ ëª¨ë“œ: {mode}")
                    logger.error("   ì‚¬ìš© ê°€ëŠ¥: full, parsing, chunking, embedding")
                    return None
                args['mode'] = mode
                i += 2
            elif tokens[i] in ['--file', '-f'] and i + 1 < len(tokens):
                args['file'] = tokens[i + 1]
                i += 2
            else:
                i += 1
        
        # í•„ìˆ˜ ì¸ì ì²´í¬
        if 'mode' not in args:
            logger.error(" --mode (-m) ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None
        if 'file' not in args:
            logger.error(" --file (-f) ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None
        
        return args
    
    def _is_guidebook(self, filename):
        if filename == 'all':
            return None  # allì€ í˜¼í•© íƒ€ì…
        return 'guidebook' in filename.lower()
    
    def _run_full_pipeline(self, filename):
        logger.info(" ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        self._run_parsing(filename)
        
        # íŒŒì‹± ê²°ê³¼ë¥¼ ì²­í‚¹ ì…ë ¥ìœ¼ë¡œ
        if filename == 'all':
            chunking_file = 'all'
        else:
            chunking_file = filename.replace('.pdf', '.json')
        
        self._run_chunking(chunking_file)
        
        # ì²­í‚¹ ê²°ê³¼ë¥¼ ì„ë² ë”© ì…ë ¥ìœ¼ë¡œ
        if filename == 'all':
            embedding_file = 'all'
        else:
            embedding_file = filename.replace('.pdf', '_chunks.jsonl')
        
        self._run_embedding(embedding_file)
    
    def _run_parsing(self, filename):
        from ingestion.parsers.standard_contract_parser import StandardContractParser
        # from ingestion.parsers.guidebook_parser import GuidebookParser
        
        logger.info(" === 1ë‹¨ê³„: íŒŒì‹± ì‹œì‘ ===")
        logger.info(f"  ì…ë ¥: {self.source_path}")
        logger.info(f"  ì¶œë ¥: {self.extracted_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.extracted_path.mkdir(parents=True, exist_ok=True)
        
        if filename == 'all':
            # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
            pattern = "*.pdf"
            files = list(self.source_path.glob(pattern))
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {len(files)}ê°œ")
            
            for file in files:
                is_guidebook = self._is_guidebook(file.name)
                parser_type = "í™œìš©ì•ˆë‚´ì„œ íŒŒì„œ" if is_guidebook else "í‘œì¤€ê³„ì•½ì„œ íŒŒì„œ"
                logger.info(f"    - {file.name} ({parser_type})")
                
                if is_guidebook:
                    logger.warning(f"        í™œìš©ì•ˆë‚´ì„œ íŒŒì„œ(ë¯¸êµ¬í˜„)")
                    continue
                else:
                    # í‘œì¤€ê³„ì•½ì„œ íŒŒì‹±
                    try:
                        parser = StandardContractParser()
                        parser.parse(file, self.extracted_path)
                        logger.info(f"       íŒŒì‹± ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"       íŒŒì‹± ì‹¤íŒ¨: {e}")
        else:
            # íŠ¹ì • íŒŒì¼ ì²˜ë¦¬
            file_path = self.source_path / filename
            if not file_path.exists():
                logger.error(f"   íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
                return
            
            is_guidebook = self._is_guidebook(filename)
            parser_type = "í™œìš©ì•ˆë‚´ì„œ íŒŒì„œ" if is_guidebook else "í‘œì¤€ê³„ì•½ì„œ íŒŒì„œ"
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {filename}")
            logger.info(f"  ì‚¬ìš© íŒŒì„œ: {parser_type}")
            
            if is_guidebook:
                logger.error(f"   í™œìš©ì•ˆë‚´ì„œ íŒŒì„œ(ë¯¸êµ¬í˜„)")
                return
            else:
                # í‘œì¤€ê³„ì•½ì„œ íŒŒì‹±
                try:
                    parser = StandardContractParser()
                    parser.parse(file_path, self.extracted_path)
                    logger.info(f"   íŒŒì‹± ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"   íŒŒì‹± ì‹¤íŒ¨: {e}")
    
    def _run_chunking(self, filename):
        logger.info("  === 2ë‹¨ê³„: ì²­í‚¹ ì‹œì‘ ===")
        logger.info(f"  ì…ë ¥: {self.extracted_path}")
        logger.info(f"  ì¶œë ¥: {self.chunked_path}")
        
        # TODO: ì²­í‚¹ ë¡œì§ êµ¬í˜„
        # from ingestion.processors.chunker import TextChunker
        
        if filename == 'all':
            pattern = "*.json"
            files = list(self.extracted_path.glob(pattern))
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {len(files)}ê°œ")
            for file in files:
                is_guidebook = self._is_guidebook(file.name)
                chunker_type = "í™œìš©ì•ˆë‚´ì„œ ì²­ì»¤" if is_guidebook else "í‘œì¤€ê³„ì•½ì„œ ì²­ì»¤"
                logger.info(f"    - {file.name} ({chunker_type})")
                # TODO: ì²­í‚¹ ì „ëµ ì„ íƒ
        else:
            file_path = self.extracted_path / filename
            if not file_path.exists():
                logger.error(f"   íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
                return
            
            is_guidebook = self._is_guidebook(filename)
            chunker_type = "í™œìš©ì•ˆë‚´ì„œ ì²­ì»¤" if is_guidebook else "í‘œì¤€ê³„ì•½ì„œ ì²­ì»¤"
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {filename}")
            logger.info(f"  ì‚¬ìš© ì²­ì»¤: {chunker_type}")
            
            # TODO: ì²­í‚¹ ì „ëµ ì„ íƒ
        
        # TODO: ì²­í‚¹ ë¡œì§
        pass
    
    def _run_embedding(self, filename):
        """ì„ë² ë”© + ì¸ë±ì‹± ì‹¤í–‰"""
        logger.info(" === 3ë‹¨ê³„: ì„ë² ë”© ì‹œì‘ ===")
        logger.info(f"  ì…ë ¥: {self.chunked_path}")
        
        # TODO: ì„ë² ë”© ë¡œì§ êµ¬í˜„
        # from ingestion.processors.embedder import TextEmbedder
        
        if filename == 'all':
            pattern = "*.jsonl"
            files = list(self.chunked_path.glob(pattern))
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {len(files)}ê°œ")
            for file in files:
                is_guidebook = self._is_guidebook(file.name)
                doc_type = "í™œìš©ì•ˆë‚´ì„œ" if is_guidebook else "í‘œì¤€ê³„ì•½ì„œ"
                logger.info(f"    - {file.name} ({doc_type})")
        else:
            file_path = self.chunked_path / filename
            if not file_path.exists():
                logger.error(f"   íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
                return
            
            is_guidebook = self._is_guidebook(filename)
            doc_type = "í™œìš©ì•ˆë‚´ì„œ" if is_guidebook else "í‘œì¤€ê³„ì•½ì„œ"
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {filename}")
            logger.info(f"  ë¬¸ì„œ íƒ€ì…: {doc_type}")
        
        # TODO: ì„ë² ë”© ë¡œì§ (ë™ì¼í•œ ì„ë² ë” ì‚¬ìš©)
        pass
        
        logger.info("ğŸ” === 4ë‹¨ê³„: ì¸ë±ì‹± ì‹œì‘ ===")
        logger.info(f"  ì¶œë ¥: {self.index_path}")
        
        # TODO: ì¸ë±ì‹± ë¡œì§ (Whoosh + FAISS)
        pass
    
    def do_status(self, arg):
        """
        í˜„ì¬ ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
        
        ì‚¬ìš©ë²•:
          status
          status --detail
        """
        logger.info("=== ë””ë ‰í† ë¦¬ ìƒíƒœ ===")
        
        # source_documents
        pdf_files = list(self.source_path.glob("*.pdf")) if self.source_path.exists() else []
        logger.info(f"\n\nì›ë³¸ PDF ({self.source_path}):")
        logger.info(f"  ì´ {len(pdf_files)}ê°œ íŒŒì¼")
        if '--detail' in arg:
            for f in pdf_files:
                logger.info(f"    - {f.name}")
        
        # extracted_documents
        json_files = list(self.extracted_path.glob("*.json")) if self.extracted_path.exists() else []
        logger.info(f"\n\níŒŒì‹± ê²°ê³¼ ({self.extracted_path}):")
        logger.info(f"  ì´ {len(json_files)}ê°œ íŒŒì¼")
        if '--detail' in arg:
            for f in json_files:
                logger.info(f"    - {f.name}")
        
        # chunked_documents
        jsonl_files = list(self.chunked_path.glob("*.jsonl")) if self.chunked_path.exists() else []
        logger.info(f"\n\nì²­í‚¹ ê²°ê³¼ ({self.chunked_path}):")
        logger.info(f"  ì´ {len(jsonl_files)}ê°œ íŒŒì¼")
        if '--detail' in arg:
            for f in jsonl_files:
                logger.info(f"    - {f.name}")
        
        # search_indexes
        has_whoosh = (self.index_path / "whoosh").exists()
        has_faiss = (self.index_path / "faiss").exists()
        logger.info(f"\n\nê²€ìƒ‰ ì¸ë±ìŠ¤ ({self.index_path}):")
        logger.info(f"  Whoosh: {'âœ…' if has_whoosh else 'âŒ'}")
        logger.info(f"  FAISS: {'âœ…' if has_faiss else 'âŒ'}")
    
    def do_ls(self, arg):
        """
        íŒŒì¼ ëª©ë¡ ë³´ê¸° (ë³„ì¹­: list)
        
        ì‚¬ìš©ë²•:
          ls <ë””ë ‰í† ë¦¬>
          
        ë””ë ‰í† ë¦¬:
          - source    : ì›ë³¸ PDF
          - extracted : íŒŒì‹± ê²°ê³¼
          - chunked   : ì²­í‚¹ ê²°ê³¼
          - index     : ì¸ë±ìŠ¤
        """
        if not arg:
            logger.error(" ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš” (source, extracted, chunked, index)")
            return
        
        path_map = {
            'source': self.source_path,
            'extracted': self.extracted_path,
            'chunked': self.chunked_path,
            'index': self.index_path
        }
        
        if arg not in path_map:
            logger.error(f" ì˜ëª»ëœ ë””ë ‰í† ë¦¬: {arg}")
            return
        
        target_path = path_map[arg]
        if not target_path.exists():
            logger.warning(f"  ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {target_path}")
            return
        
        logger.info(f"\n {target_path}:")
        files = sorted(target_path.iterdir())
        for f in files:
            if f.is_file():
                size_kb = f.stat().st_size / 1024
                logger.info(f"  {f.name} ({size_kb:.1f} KB)")
            elif f.is_dir():
                logger.info(f"   {f.name}/")
    
    def do_exit(self, arg):
        logger.info("ingestion ì¢…ë£Œ")
        return True
    
    def emptyline(self):
        pass
    
    def default(self, line):
        logger.error(f" ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ {line}")
        logger.info(" 'help'ë¥¼ ì…ë ¥í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´ í™•ì¸")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    cli = IngestionCLI()
    try:
        cli.cmdloop()
    except KeyboardInterrupt:
        logger.info("\n\ningestion ì¢…ë£Œ")
        sys.exit(0)


if __name__ == "__main__":
    main()
