import cmd
import os
import sys
from pathlib import Path
from typing import Optional
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# python ingestion/ingest.py
# verify -u data/user_contract_sample_1.txt


class IngestionCLI(cmd.Cmd):
    """ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• CLI ëª¨ë“ˆ"""
    
    intro = """

Commands:
  - run     : ì‘ì—… ì‹¤í–‰
  - search  : FAISS ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
  - verify  : ê³„ì•½ì„œ ê²€ì¦ (í‘œì¤€ ê³„ì•½ì„œ ëŒ€ë¹„ ì‚¬ìš©ì ê³„ì•½ì„œ ê²€ì¦)
  - status  : ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
  - help    : ë„ì›€ë§
  - exit    : ì¢…ë£Œ

"""
    prompt = ' ingestion> '
    
    def __init__(self):
        super().__init__()
        # ê²½ë¡œ ì„¤ì •
        self.base_path = Path("/app/data")
        self.source_path = self.base_path / "source_documents"
        self.extracted_path = self.base_path / "extracted_documents"
        self.chunked_path = self.base_path / "chunked_documents"
        self.index_path = Path("/app/search_indexes")
    
    def do_run(self, arg):
        """
        ì‘ì—… ì‹¤í–‰
        
        ì‚¬ìš©ë²•:
          run --mode <mode> --file <filename>
          run -m <mode> -f <filename>
          
        ì˜ˆì‹œ:
          run --mode parsing --file create_std_contract.pdf
          run -m parsing -f create_std_contract.docx
          run -m full -f all
          run --mode chunking --file create_std_contract.json
          run -m embedding -f create_std_contract_chunks.jsonl
        
        --mode ì˜µì…˜:
          - full        : ì „ì²´ íŒŒì´í”„ë¼ì¸ (íŒŒì‹±â†’ì²­í‚¹â†’ì„ë² ë”©â†’ì¸ë±ì‹±)
          - parsing     : ë¬¸ì„œ íŒŒì‹±ë§Œ (PDF/DOCX ìë™ ê°ì§€)
          - chunking    : JSON ì²­í‚¹ë§Œ
          - embedding   : ì„ë² ë”© + ì¸ë±ì‹±
          - s_embedding : ê°„ì´ ì²­í‚¹ ë° ì„ë² ë”© (ì¡°/ë³„ì§€ ë‹¨ìœ„)
        
        --file ì˜µì…˜:
          - all             : ëª¨ë“  íŒŒì¼ (PDF, DOCX ëª¨ë‘)
          - <filename>      : íŠ¹ì • íŒŒì¼ í•˜ë‚˜
        
        ì°¸ê³ :
          - íŒŒì¼ í™•ì¥ì ê°ì§€ë¡œ íŒŒì„œ ìë™ ì„ íƒ
          - íŒŒì¼ëª…ì— 'guidebook' í¬í•¨ â†’ í™œìš©ì•ˆë‚´ì„œ ëª¨ë“ˆ ì‚¬ìš©
          - ê·¸ ì™¸ íŒŒì¼ â†’ í‘œì¤€ê³„ì•½ì„œ ëª¨ë“ˆ ì‚¬ìš©
        """
        try:
            # ì¸ì íŒŒì‹±
            args = self._parse_run_args(arg)
            if not args:
                return
            
            mode = args.get('mode')
            filename = args.get('file')
            
            logger.info("=" * 60)
            logger.info(f" ì‘ì—… ì‹œì‘")
            logger.info(f"  ëª¨ë“œ: {mode}")
            logger.info(f"  íŒŒì¼: {filename}")
            logger.info("=" * 60)
            
            # ëª¨ë“œë³„ ì‹¤í–‰
            if mode == 'full':
                self._run_full_pipeline(filename)
            elif mode == 'parsing':
                self._run_parsing(filename)
            elif mode == 'chunking':
                self._run_chunking(filename)
            elif mode == 'embedding':
                self._run_embedding(filename)
            elif mode == 's_embedding':
                self._run_simple_embedding(filename)
            else:
                logger.error(f" ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {mode}")
                return
            
            logger.info("=" * 60)
            logger.info(" ì‘ì—… ì™„ë£Œ")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f" ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    def _parse_run_args(self, arg):
        """run ëª…ë ¹ì–´ ì¸ì íŒŒì‹±"""
        args = {}
        tokens = arg.split()
        
        i = 0
        while i < len(tokens):
            if tokens[i] in ['--mode', '-m'] and i + 1 < len(tokens):
                mode = tokens[i + 1]
                if mode not in ['full', 'parsing', 'chunking', 'embedding', 's_embedding']:
                    logger.error(f" ì˜ëª»ëœ ëª¨ë“œ: {mode}")
                    logger.error("   ì‚¬ìš© ê°€ëŠ¥: full, parsing, chunking, embedding, s_embedding")
                    return None
                args['mode'] = mode
                i += 2
            elif tokens[i] in ['--file', '-f'] and i + 1 < len(tokens):
                args['file'] = tokens[i + 1]
                i += 2
            else:
                i += 1
        
        # í•„ìˆ˜ ì¸ì ì²´í¬
        if 'mode' not in args:
            logger.error(" --mode (-m) ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None
        if 'file' not in args:
            logger.error(" --file (-f) ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None
        
        return args
    
    def _is_guidebook(self, filename):
        if filename == 'all':
            return None  # allì€ í˜¼í•© íƒ€ì…
        return 'guidebook' in filename.lower()
    
    def _run_full_pipeline(self, filename):
        logger.info("=== ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ===")
        self._run_parsing(filename)
        
        # íŒŒì‹± ê²°ê³¼ë¥¼ ì²­í‚¹ ì…ë ¥ìœ¼ë¡œ
        if filename == 'all':
            chunking_file = 'all'
        else:
            # .pdf ë˜ëŠ” .docxë¥¼ .jsonìœ¼ë¡œ ë³€í™˜
            chunking_file = filename.replace('.pdf', '.json').replace('.docx', '_structured.json')
        
        self._run_chunking(chunking_file)
        
        # ì²­í‚¹ ê²°ê³¼ë¥¼ ì„ë² ë”© ì…ë ¥ìœ¼ë¡œ
        if filename == 'all':
            embedding_file = 'all'
        else:
            # í™•ì¥ì ì œê±° í›„ _chunks.jsonl ì¶”ê°€
            base_name = filename.rsplit('.', 1)[0]
            embedding_file = f"{base_name}_chunks.jsonl"
        
        self._run_embedding(embedding_file)
    
    def _get_parser(self, filename: str, file_ext: str):
        """
        íŒŒì¼ëª…ê³¼ í™•ì¥ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ íŒŒì„œ ì„ íƒ
        
        Args:
            filename: íŒŒì¼ëª…
            file_ext: íŒŒì¼ í™•ì¥ì (.pdf, .docx ë“±)
            
        Returns:
            íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤
        """
        is_guidebook = self._is_guidebook(filename)
        
        # í™•ì¥ìì™€ ë¬¸ì„œ ìœ í˜•ì— ë”°ë¼ íŒŒì„œ ì„ íƒ
        if file_ext == '.pdf':
            if is_guidebook:
                from ingestion.parsers.guidebook_pdf_parser import GuidebookPdfParser
                return GuidebookPdfParser(), "í™œìš©ì•ˆë‚´ì„œ PDF íŒŒì„œ"
            else:
                from ingestion.parsers.std_contract_pdf_parser import StdContractPdfParser
                return StdContractPdfParser(), "í‘œì¤€ê³„ì•½ì„œ PDF íŒŒì„œ"
        
        elif file_ext == '.docx':
            if is_guidebook:
                from ingestion.parsers.guidebook_docx_parser import GuidebookDocxParser
                return GuidebookDocxParser(), "í™œìš©ì•ˆë‚´ì„œ DOCX íŒŒì„œ"
            else:
                from ingestion.parsers.std_contract_docx_parser import StdContractDocxParser
                return StdContractDocxParser(), "í‘œì¤€ê³„ì•½ì„œ DOCX íŒŒì„œ"
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")
    
    def _run_parsing(self, filename):
        logger.info("=== 1ë‹¨ê³„: íŒŒì‹± ì‹œì‘ ===")
        logger.info(f"  ì…ë ¥: {self.source_path}")
        logger.info(f"  ì¶œë ¥: {self.extracted_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.extracted_path.mkdir(parents=True, exist_ok=True)
        
        if filename == 'all':
            # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ (PDFì™€ DOCX)
            pdf_files = list(self.source_path.glob("*.pdf"))
            docx_files = list(self.source_path.glob("*.docx"))
            all_files = pdf_files + docx_files
            
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {len(all_files)}ê°œ (PDF: {len(pdf_files)}, DOCX: {len(docx_files)})")
            
            for file in all_files:
                file_ext = file.suffix.lower()
                
                try:
                    parser, parser_name = self._get_parser(file.name, file_ext)
                    logger.info(f"    - {file.name} ({parser_name})")
                    
                    parser.parse(file, self.extracted_path)
                    logger.info(f"        íŒŒì‹± ì™„ë£Œ")
                    
                except ValueError as e:
                    logger.error(f"       [ERROR] {e}")
                except Exception as e:
                    logger.error(f"       [ERROR] íŒŒì‹± ì‹¤íŒ¨: {e}")
                    import traceback
                    traceback.print_exc()
        else:
            # íŠ¹ì • íŒŒì¼ ì²˜ë¦¬
            file_path = self.source_path / filename
            if not file_path.exists():
                logger.error(f"   [ERROR] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
                return
            
            file_ext = file_path.suffix.lower()
            
            try:
                parser, parser_name = self._get_parser(filename, file_ext)
                logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {filename}")
                logger.info(f"  ì‚¬ìš© íŒŒì„œ: {parser_name}")
                
                parser.parse(file_path, self.extracted_path)
                logger.info(f"   [OK] íŒŒì‹± ì™„ë£Œ")
                
            except ValueError as e:
                logger.error(f"   [ERROR] {e}")
            except Exception as e:
                logger.error(f"   [ERROR] íŒŒì‹± ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
    
    def _run_chunking(self, filename):
        logger.info("=== 2ë‹¨ê³„: ì²­í‚¹ ì‹œì‘ ===")
        logger.info(f"  ì…ë ¥: {self.extracted_path}")
        logger.info(f"  ì¶œë ¥: {self.chunked_path}")
        
        from ingestion.processors.chunker import TextChunker
        chunker = TextChunker(base_dir=self.base_path)
        
        def process_single(target_path: Path):
            try:
                chunker.process_file(target_path.name)
                logger.info(f"   [OK] ì²­í‚¹ ì™„ë£Œ: {target_path.name}")
            except Exception as e:
                logger.error(f"   [ERROR] ì²­í‚¹ ì‹¤íŒ¨: {target_path.name} - {e}")
                import traceback
                traceback.print_exc()
        
        if filename == 'all':
            files = sorted(self.extracted_path.glob("*.json"))
            if not files:
                logger.warning("  ì²˜ë¦¬í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {len(files)}ê°œ")
            for file in files:
                is_guidebook = self._is_guidebook(file.name)
                chunker_type = "í™œìš© ì•ˆë‚´ì„œ ì²­ì»¤" if is_guidebook else "í‘œì¤€ê³„ì•½ì„œ ì²­ì»¤"
                logger.info(f"    - {file.name} ({chunker_type})")
                process_single(file)
        else:
            file_path = self.extracted_path / filename
            if not file_path.exists():
                logger.error(f"   íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
                return
            
            is_guidebook = self._is_guidebook(filename)
            chunker_type = "í™œìš© ì•ˆë‚´ì„œ ì²­ì»¤" if is_guidebook else "í‘œì¤€ê³„ì•½ì„œ ì²­ì»¤"
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {filename}")
            logger.info(f"  ì‚¬ìš© ì²­ì»¤: {chunker_type}")
            
            process_single(file_path)
    def _run_embedding(self, filename):
        """ì„ë² ë”© + ì¸ë±ì‹± ì‹¤í–‰"""
        logger.info("=== 3ë‹¨ê³„: ì„ë² ë”© ì‹œì‘ ===")
        logger.info(f"  ì…ë ¥: {self.chunked_path}")
        
        # TODO: ì„ë² ë”© ë¡œì§ êµ¬í˜„
        # from ingestion.processors.embedder import TextEmbedder
        
        if filename == 'all':
            pattern = "*.jsonl"
            files = list(self.chunked_path.glob(pattern))
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {len(files)}ê°œ")
            for file in files:
                is_guidebook = self._is_guidebook(file.name)
                doc_type = "í™œìš©ì•ˆë‚´ì„œ" if is_guidebook else "í‘œì¤€ê³„ì•½ì„œ"
                logger.info(f"    - {file.name} ({doc_type})")
        else:
            file_path = self.chunked_path / filename
            if not file_path.exists():
                logger.error(f"   íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
                return
            
            is_guidebook = self._is_guidebook(filename)
            doc_type = "í™œìš©ì•ˆë‚´ì„œ" if is_guidebook else "í‘œì¤€ê³„ì•½ì„œ"
            logger.info(f"  ì²˜ë¦¬í•  íŒŒì¼: {filename}")
            logger.info(f"  ë¬¸ì„œ íƒ€ì…: {doc_type}")
        
        # TODO: ì„ë² ë”© ë¡œì§ (ë™ì¼í•œ ì„ë² ë” ì‚¬ìš©)
        pass
        
        logger.info("=== 4ë‹¨ê³„: ì¸ë±ì‹± ì‹œì‘ ===")
        logger.info(f"  ì¶œë ¥: {self.index_path}")
        
        # TODO: ì¸ë±ì‹± ë¡œì§ (Whoosh + FAISS)
        pass
    
    def _run_simple_embedding(self, filename):
        """
        ê°„ì´ ì²­í‚¹ ë° ì„ë² ë”© ì‹¤í–‰
        ì¡°/ë³„ì§€ ë‹¨ìœ„ë¡œ ì²­í‚¹í•˜ê³  Azure OpenAI ì„ë² ë”© ìƒì„± í›„ FAISSì— ì €ì¥
        """
        import os
        from ingestion.processors.s_embedder import SimpleEmbedder
        
        logger.info("=== ê°„ì´ ì²­í‚¹ ë° ì„ë² ë”© ì‹œì‘ ===")
        logger.info(f"  ì…ë ¥: {self.extracted_path}")
        logger.info(f"  ì¶œë ¥: {self.index_path}")
        
        # Azure OpenAI API í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
        api_key = os.getenv('AZURE_OPENAI_API_KEY')
        azure_endpoint = os.getenv('AZURE_ENDPOINT')
        
        if not api_key:
            logger.error("   [ERROR] AZURE_OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return
        
        if not azure_endpoint:
            logger.error("   [ERROR] AZURE_ENDPOINT í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return
        
        # structured.json íŒŒì¼ ê²½ë¡œ í™•ì¸
        file_path = self.extracted_path / filename
        if not file_path.exists():
            logger.error(f"   [ERROR] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
            return
        
        # Azure OpenAI deployment name í™•ì¸ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’ ì‚¬ìš© ê°€ëŠ¥)
        deployment_name = os.getenv('AZURE_EMBEDDING_DEPLOYMENT', 'text-embedding-3-large')
        
        logger.info(f"  Azure Endpoint: {azure_endpoint}")
        logger.info(f"  Deployment Name: {deployment_name}")
        
        # SimpleEmbedderë¡œ ì²˜ë¦¬
        embedder = SimpleEmbedder(
            api_key=api_key,
            azure_endpoint=azure_endpoint,
            model=deployment_name
        )
        faiss_output_dir = self.index_path / "faiss"
        
        success = embedder.process_file(file_path, faiss_output_dir)
        
        if not success:
            logger.error("   [ERROR] ê°„ì´ ì²­í‚¹ ë° ì„ë² ë”© ì‹¤íŒ¨")
            return
    
    def do_search(self, arg):
        """
        FAISS ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        
        ì‚¬ìš©ë²•:
          search --index <index_name> --query <query_text>
          search -i <index_name> -q <query_text>
          search -i <index_name> -q <query_text> --top <k>
          
        ì˜ˆì‹œ:
          search -i provide_std_contract -q "ì§ˆì˜"
          search -i provide_std_contract -q "ì§ˆì˜" --top 3
        
        --index ì˜µì…˜:
          - FAISS ì¸ë±ìŠ¤ ì´ë¦„
          - ì˜ˆ: provide_std_contract
        
        --query ì˜µì…˜:
          - ê²€ìƒ‰í•  ì§ˆë¬¸
          
        --top ì˜µì…˜ (ì„ íƒ):
          - ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
        """
        try:
            import os
            from ingestion.processors.s_searcher import SimpleSearcher
            
            # ì¸ì íŒŒì‹±
            args = self._parse_search_args(arg)
            if not args:
                return
            
            index_name = args.get('index')
            query = args.get('query')
            top_k = args.get('top', 5)
            
            logger.info("=" * 60)
            logger.info(" ê°„ì´ RAG ê²€ìƒ‰ ì‹œì‘")
            logger.info(f"  ì¸ë±ìŠ¤: {index_name}")
            logger.info(f"  Top-K: {top_k}")
            logger.info("=" * 60)
            
            # Azure OpenAI API í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
            api_key = os.getenv('AZURE_OPENAI_API_KEY')
            azure_endpoint = os.getenv('AZURE_ENDPOINT')
            deployment_name = os.getenv('AZURE_EMBEDDING_DEPLOYMENT', 'text-embedding-3-large')
            
            if not api_key or not azure_endpoint:
                logger.error("   [ERROR] Azure OpenAI í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return
            
            # SimpleSearcher ì´ˆê¸°í™”
            searcher = SimpleSearcher(
                api_key=api_key,
                azure_endpoint=azure_endpoint,
                embedding_model=deployment_name
            )
            
            # ì¸ë±ìŠ¤ ë¡œë“œ
            faiss_dir = self.index_path / "faiss"
            if not searcher.load_index(faiss_dir, index_name):
                return
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            results = searcher.search(query, top_k=top_k)
            
            # ê²°ê³¼ í‘œì‹œ
            searcher.display_results(results)
            
            # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (LLM ì‚¬ìš© ì‹œ í™œìš© ê°€ëŠ¥)
            if results:
                context = searcher.get_context(results)
                logger.info(f"  [INFO] LLMìš© ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ë¬¸ì")
            
            logger.info("=" * 60)
            logger.info(" ê²€ìƒ‰ ì™„ë£Œ")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f" ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    def _parse_search_args(self, arg):
        """search ëª…ë ¹ì–´ ì¸ì íŒŒì‹±"""
        args = {}
        tokens = arg.split()
        
        i = 0
        query_tokens = []
        collecting_query = False
        
        while i < len(tokens):
            if tokens[i] in ['--index', '-i'] and i + 1 < len(tokens):
                args['index'] = tokens[i + 1]
                i += 2
            elif tokens[i] in ['--query', '-q']:
                collecting_query = True
                i += 1
            elif tokens[i] in ['--top', '-t'] and i + 1 < len(tokens):
                try:
                    args['top'] = int(tokens[i + 1])
                except ValueError:
                    logger.error(f" --top ê°’ì€ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤: {tokens[i + 1]}")
                    return None
                collecting_query = False
                i += 2
            elif collecting_query:
                # --topì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ëª¨ë“  í† í°ì„ ì¿¼ë¦¬ë¡œ ìˆ˜ì§‘
                if tokens[i] in ['--top', '-t']:
                    collecting_query = False
                    continue
                query_tokens.append(tokens[i])
                i += 1
            else:
                i += 1
        
        # ì¿¼ë¦¬ ì¡°ë¦½
        if query_tokens:
            args['query'] = ' '.join(query_tokens)
        
        # í•„ìˆ˜ ì¸ì ì²´í¬
        if 'index' not in args:
            logger.error(" --index (-i) ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None
        if 'query' not in args:
            logger.error(" --query (-q) ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None
        
        return args
    
    def do_status(self, arg):
        """
        í˜„ì¬ ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
        
        ì‚¬ìš©ë²•:
          status
          status --detail
        """
        logger.info("=== ë””ë ‰í† ë¦¬ ìƒíƒœ ===")
        
        # source_documents
        pdf_files = list(self.source_path.glob("*.pdf")) if self.source_path.exists() else []
        docx_files = list(self.source_path.glob("*.docx")) if self.source_path.exists() else []
        logger.info(f"\n [ì›ë³¸ ë¬¸ì„œ] ({self.source_path}):")
        logger.info(f"  ì´ {len(pdf_files) + len(docx_files)}ê°œ íŒŒì¼ (PDF: {len(pdf_files)}, DOCX: {len(docx_files)})")
        if '--detail' in arg:
            for f in pdf_files + docx_files:
                logger.info(f"    - {f.name}")
        
        # extracted_documents
        json_files = list(self.extracted_path.glob("*.json")) if self.extracted_path.exists() else []
        logger.info(f"\n [íŒŒì‹± ê²°ê³¼] ({self.extracted_path}):")
        logger.info(f"  ì´ {len(json_files)}ê°œ íŒŒì¼")
        if '--detail' in arg:
            for f in json_files:
                logger.info(f"    - {f.name}")
        
        # chunked_documents
        jsonl_files = list(self.chunked_path.glob("*.jsonl")) if self.chunked_path.exists() else []
        logger.info(f"\n [ì²­í‚¹ ê²°ê³¼] ({self.chunked_path}):")
        logger.info(f"  ì´ {len(jsonl_files)}ê°œ íŒŒì¼")
        if '--detail' in arg:
            for f in jsonl_files:
                logger.info(f"    - {f.name}")
        
        # search_indexes
        whoosh_status = self._check_whoosh_index()
        faiss_status = self._check_faiss_index()
        
        logger.info(f"\n [ê²€ìƒ‰ ì¸ë±ìŠ¤] ({self.index_path}):")
        logger.info(f"  Whoosh: {whoosh_status['icon']} {whoosh_status['message']}")
        logger.info(f"  FAISS: {faiss_status['icon']} {faiss_status['message']}")
    
    def _check_whoosh_index(self) -> dict:
        """
        Whoosh ì¸ë±ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
        
        Returns:
            dict: {"icon": str, "message": str, "exists": bool}
        """
        whoosh_dir = self.index_path / "whoosh"
        
        # Whoosh ì¸ë±ìŠ¤ í•„ìˆ˜ íŒŒì¼ ì²´í¬
        # _MAIN_*.toc íŒŒì¼ì´ ìˆìœ¼ë©´ ì¸ë±ìŠ¤ê°€ ìƒì„±ëœ ê²ƒ
        toc_files = list(whoosh_dir.glob("_MAIN_*.toc"))
        
        if not toc_files:
            return {"icon": "X", "message": "ì¸ë±ìŠ¤ ì—†ìŒ", "exists": False}
        
        # ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ë„ í™•ì¸
        seg_files = list(whoosh_dir.glob("*.seg"))
        
        if toc_files and seg_files:
            return {"icon": "O", "message": f"ì¤€ë¹„ë¨ ({len(toc_files)}ê°œ TOC, {len(seg_files)}ê°œ ì„¸ê·¸ë¨¼íŠ¸)", "exists": True}
        else:
            return {"icon": "!", "message": "ì¸ë±ìŠ¤ ë¶ˆì™„ì „ (ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ ì—†ìŒ)", "exists": False}
    
    def _check_faiss_index(self) -> dict:
        """
        FAISS ì¸ë±ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
        
        Returns:
            dict: {"icon": str, "message": str, "exists": bool}
        """
        faiss_dir = self.index_path / "faiss"
        
        # FAISS ì¸ë±ìŠ¤ í•„ìˆ˜ íŒŒì¼ ì²´í¬
        # ì¼ë°˜ì ìœ¼ë¡œ .index ë˜ëŠ” .faiss í™•ì¥ì íŒŒì¼
        index_files = list(faiss_dir.glob("*.index")) + list(faiss_dir.glob("*.faiss"))
        
        if not index_files:
            return {"icon": "X", "message": "ì¸ë±ìŠ¤ ì—†ìŒ", "exists": False}
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ë„ í™•ì¸ (ì„ íƒì )
        metadata_files = list(faiss_dir.glob("*.json")) + list(faiss_dir.glob("*.pkl"))
        
        total_size = sum(f.stat().st_size for f in index_files) / (1024 * 1024)  # MB
        
        msg = f"ì¤€ë¹„ë¨ ({len(index_files)}ê°œ íŒŒì¼, {total_size:.1f}MB"
        if metadata_files:
            msg += f", ë©”íƒ€ë°ì´í„° {len(metadata_files)}ê°œ"
        msg += ")"
        return {"icon": "O", "message": msg, "exists": True}
    
    def do_ls(self, arg):
        """
        íŒŒì¼ ëª©ë¡ ë³´ê¸° (ë³„ì¹­: list)
        
        ì‚¬ìš©ë²•:
          ls <ë””ë ‰í† ë¦¬>
          
        ë””ë ‰í† ë¦¬:
          - source    : ì›ë³¸ PDF
          - extracted : íŒŒì‹± ê²°ê³¼
          - chunked   : ì²­í‚¹ ê²°ê³¼
          - index     : ì¸ë±ìŠ¤
        """
        if not arg:
            logger.error(" ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš” (source, extracted, chunked, index)")
            return
        
        path_map = {
            'source': self.source_path,
            'extracted': self.extracted_path,
            'chunked': self.chunked_path,
            'index': self.index_path
        }
        
        if arg not in path_map:
            logger.error(f" ì˜ëª»ëœ ë””ë ‰í† ë¦¬: {arg}")
            return
        
        target_path = path_map[arg]
        if not target_path.exists():
            logger.warning(f"  ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {target_path}")
            return
        
        logger.info(f"\n {target_path}:")
        files = sorted(target_path.iterdir())
        for f in files:
            if f.is_file():
                size_kb = f.stat().st_size / 1024
                logger.info(f"  {f.name} ({size_kb:.1f} KB)")
            elif f.is_dir():
                logger.info(f"   {f.name}/")
    
    def do_verify(self, arg):
        """
        ê³„ì•½ì„œ ê²€ì¦ ì‹¤í–‰
        
        ì‚¬ìš©ë²•:
          verify --user <user_contract_path>
          verify -u <user_contract_path>
          verify -u <user_contract_path> --format <format>
          
        ì˜ˆì‹œ:
          verify -u data/user_contract.txt
          verify -u data/user_contract.docx
          verify -u data/user_contract.docx --format pdf
          verify -u data/user_contract.txt --format both
        
        --user ì˜µì…˜:
          - ì‚¬ìš©ì ê³„ì•½ì„œ íŒŒì¼ ê²½ë¡œ (.txt ë˜ëŠ” .docx íŒŒì¼)
          
        --format ì˜µì…˜ (ì„ íƒ):
          - text : í…ìŠ¤íŠ¸ ë³´ê³ ì„œë§Œ ìƒì„± (ê¸°ë³¸ê°’)
          - pdf  : PDF ë³´ê³ ì„œë§Œ ìƒì„±
          - both : í…ìŠ¤íŠ¸ + PDF ë³´ê³ ì„œ ëª¨ë‘ ìƒì„±
        
        ì°¸ê³ :
          - í‘œì¤€ ê³„ì•½ì„œëŠ” ìë™ìœ¼ë¡œ data/chunked_documents/provide_std_contract_chunks.json ì‚¬ìš©
          - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + FAISS) + LLM ê²€ì¦ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
          - í•­ ë‹¨ìœ„ ì²­í‚¹ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
          - ë³´ê³ ì„œëŠ” data/reports/ ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤
        """
        try:
            # ì¸ì íŒŒì‹±
            args = self._parse_verify_args(arg)
            if not args:
                return
            
            user_contract_path = args.get('user')
            report_format = args.get('format', 'text')
            
            logger.info("=" * 60)
            logger.info(" ê³„ì•½ì„œ ê²€ì¦ ì‹œì‘")
            logger.info(f"  ì‚¬ìš©ì ê³„ì•½ì„œ: {user_contract_path}")
            logger.info(f"  ë³´ê³ ì„œ í˜•ì‹: {report_format}")
            logger.info("=" * 60)
            
            # ê²€ì¦ ì‹¤í–‰
            self._run_verification(user_contract_path, report_format)
            
            logger.info("=" * 60)
            logger.info(" ê²€ì¦ ì™„ë£Œ")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f" ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    def _parse_verify_args(self, arg):
        """verify ëª…ë ¹ì–´ ì¸ì íŒŒì‹±"""
        args = {}
        tokens = arg.split()
        
        i = 0
        while i < len(tokens):
            if tokens[i] in ['--user', '-u'] and i + 1 < len(tokens):
                args['user'] = tokens[i + 1]
                i += 2
            elif tokens[i] in ['--format', '-f'] and i + 1 < len(tokens):
                fmt = tokens[i + 1]
                if fmt not in ['text', 'pdf', 'both']:
                    logger.error(f" ì˜ëª»ëœ í˜•ì‹: {fmt}")
                    logger.error("   ì‚¬ìš© ê°€ëŠ¥: text, pdf, both")
                    return None
                args['format'] = fmt
                i += 2
            else:
                i += 1
        
        # í•„ìˆ˜ ì¸ì ì²´í¬
        if 'user' not in args:
            logger.error(" --user (-u) ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None
        
        return args
    
    def _group_results_by_article(self, match_results):
        """ë§¤ì¹­ ê²°ê³¼ë¥¼ ì¡°ë¬¸ë³„ë¡œ ê·¸ë£¹í™”"""
        article_groups = {}
        
        for match in match_results:
            if match.is_matched:
                # ì¡°ë¬¸ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: "ì œ1ì¡° (ëª©ì )" â†’ "ì œ1ì¡°")
                user_title = match.matched_clause.title
                article_num = user_title.split()[0] if user_title.startswith('ì œ') else user_title
                
                if article_num not in article_groups:
                    article_groups[article_num] = []
                
                article_groups[article_num].append(match)
        
        return article_groups
    
    def _generate_detailed_report(self, report_path, result, grouped_results, user_clauses):
        """ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„± (run_chunked_verification.pyì™€ ë™ì¼)"""
        from datetime import datetime
        
        with open(report_path, 'w', encoding='utf-8') as f:
            # í—¤ë”
            f.write("="*100 + "\n")
            f.write("ê°œì„ ëœ ê³„ì•½ì„œ ê²€ì¦ ë³´ê³ ì„œ (í•­ ë‹¨ìœ„ ì²­í‚¹ + ì¡°ë¬¸ë³„ ê·¸ë£¹í™”)\n")
            f.write("="*100 + "\n\n")
            f.write(f"ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ê²€ì¦ ë°©ì‹: í•­ ë‹¨ìœ„ ì²­í‚¹ â†’ í‘œì¤€ ì¡°ë¬¸ ë§¤ì¹­ â†’ ì¡°ë¬¸ë³„ ê·¸ë£¹í™”\n\n")
            
            # ì ìˆ˜ í•´ì„ ê°€ì´ë“œ
            f.write("ğŸ“– ì ìˆ˜ í•´ì„ ê°€ì´ë“œ\n")
            f.write("-" * 50 + "\n")
            f.write("â€¢ BM25 ì ìˆ˜: í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, 0~1)\n")
            f.write("â€¢ FAISS ìœ ì‚¬ë„: ì˜ë¯¸ì  ìœ ì‚¬ë„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, 0~100%)\n")
            f.write("  - ê³„ì‚°: exp(-(ê±°ë¦¬^2)/2) Ã— 100%\n")
            f.write("  - 80%+: ë§¤ìš° ìœ ì‚¬\n")
            f.write("  - 60~80%: ì–´ëŠ ì •ë„ ìœ ì‚¬\n")
            f.write("  - 40~60%: ìœ ì‚¬ë„ ë‚®ìŒ\n")
            f.write("  - 40% ë¯¸ë§Œ: ë§¤ìš° ë‹¤ë¦„\n")
            f.write("â€¢ í•˜ì´ë¸Œë¦¬ë“œ: BM25(20%) + FAISS(80%) ê²°í•© ì ìˆ˜\n\n")
            
            # ìš”ì•½ í†µê³„
            f.write("ğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½\n")
            f.write("-" * 50 + "\n")
            f.write(f"ì´ ì²­í¬ ìˆ˜: {len(user_clauses)}ê°œ\n")
            f.write(f"ë§¤ì¹­ëœ ì²­í¬: {result.matched_clauses}ê°œ\n")
            f.write(f"ë§¤ì¹­ë¥ : {result.matched_clauses/len(user_clauses)*100:.1f}%\n")
            f.write(f"ì¡°ë¬¸ë³„ ê·¸ë£¹: {len(grouped_results)}ê°œ\n\n")
            
            # ì¡°ë¬¸ë³„ ë§¤ì¹­ ê²°ê³¼
            f.write("ğŸ“‹ ì¡°ë¬¸ë³„ ë§¤ì¹­ ê²°ê³¼\n")
            f.write("-" * 50 + "\n")
            
            for article_num in sorted(grouped_results.keys(), key=lambda x: int(x[1:-1]) if x[1:-1].isdigit() else 999):
                matches = grouped_results[article_num]
                f.write(f"\nğŸ”¸ {article_num} ({len(matches)}ê°œ í•­ ë§¤ì¹­)\n")
                
                for match in matches:
                    f.write(f"   âœ… [{match.standard_clause.id}] {match.standard_clause.title}\n")
                    f.write(f"      â† {match.matched_clause.title}\n")
                    
                    # ë³„ì§€ ì°¸ì¡° í™•ì¸
                    if "[ë³„ì§€" in match.standard_clause.text:
                        f.write(f"      ğŸ“ ë³„ì§€ ì°¸ì¡° ìˆìŒ\n")
                    
                    if match.llm_decision:
                        f.write(f"      ì‹ ë¢°ë„: {match.llm_decision.confidence:.0%}\n")
                    f.write(f"      í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {match.hybrid_score:.3f}\n")
                    # FAISS ìœ ì‚¬ë„ ê³„ì‚°: exp(-(distance^2)/2) * 100
                    import numpy as np
                    if match.faiss_raw_distance is not None:
                        faiss_similarity = np.exp(-(match.faiss_raw_distance ** 2) / 2.0) * 100
                        f.write(f"      BM25: {match.bm25_score:.3f} | FAISS ìœ ì‚¬ë„: {faiss_similarity:.1f}%\n")
                    else:
                        f.write(f"      FAISS ì ìˆ˜: {match.faiss_score:.3f}\n")
                    f.write("\n")
            
            # ìƒì„¸ ë§¤ì¹­ ê²°ê³¼
            f.write("\n" + "="*100 + "\n")
            f.write("ğŸ“ ìƒì„¸ ë§¤ì¹­ ê²°ê³¼\n")
            f.write("="*100 + "\n\n")
            
            for i, match in enumerate([m for m in result.match_results if m.is_matched], 1):
                f.write(f"{i:2d}. ë§¤ì¹­ ì„±ê³µ\n")
                f.write(f"    í‘œì¤€ ì¡°ë¬¸: [{match.standard_clause.id}] {match.standard_clause.title}\n")
                
                # ë³„ì§€ ì°¸ì¡° í™•ì¸
                if "[ë³„ì§€" in match.standard_clause.text:
                    import re
                    appendix_refs = re.findall(r'\[ë³„ì§€(\d+)\]', match.standard_clause.text)
                    if appendix_refs:
                        f.write(f"    ğŸ“ ë³„ì§€ ì°¸ì¡°: ë³„ì§€{', ë³„ì§€'.join(appendix_refs)}\n")
                
                f.write(f"    ì‚¬ìš©ì í•­: {match.matched_clause.title}\n")
                
                if match.llm_decision:
                    f.write(f"    LLM ì‹ ë¢°ë„: {match.llm_decision.confidence:.0%}\n")
                    f.write(f"    LLM íŒë‹¨: {match.llm_decision.reasoning}\n")
                
                f.write(f"    ê²€ìƒ‰ ì ìˆ˜:\n")
                f.write(f"      - í•˜ì´ë¸Œë¦¬ë“œ: {match.hybrid_score:.3f}\n")
                # FAISS ìœ ì‚¬ë„ ê³„ì‚°
                import numpy as np
                if match.faiss_raw_distance is not None and match.bm25_raw_score is not None:
                    f.write(f"      - BM25: {match.bm25_score:.3f} (ì›ì ìˆ˜: {match.bm25_raw_score:.3f})\n")
                    faiss_similarity = np.exp(-(match.faiss_raw_distance ** 2) / 2.0) * 100
                    f.write(f"      - FAISS ìœ ì‚¬ë„: {faiss_similarity:.1f}%\n")
                else:
                    f.write(f"      - FAISS ì ìˆ˜: {match.faiss_score:.3f}\n")
                
                f.write(f"\n    í‘œì¤€ ì¡°ë¬¸ ë‚´ìš©:\n")
                f.write(f"    {match.standard_clause.text[:200]}...\n")
                
                f.write(f"\n    ì‚¬ìš©ì í•­ ë‚´ìš©:\n")
                f.write(f"    {match.matched_clause.text[:200]}...\n")
                
                f.write("\n" + "-"*80 + "\n\n")
            
            # ë§¤ì¹­ë˜ì§€ ì•Šì€ í‘œì¤€ ì¡°ë¬¸ (ëˆ„ë½ëœ ì¡°ë¬¸)
            if result.missing_clauses:
                f.write("âŒ ë§¤ì¹­ë˜ì§€ ì•Šì€ í‘œì¤€ ì¡°ë¬¸ (ëˆ„ë½)\n")
                f.write("-" * 50 + "\n")
                for clause in result.missing_clauses:
                    f.write(f"   [{clause.id}] {clause.title}\n")
                    f.write(f"   {clause.text[:100]}...\n\n")
            
            # ë§¤ì¹­ë˜ì§€ ì•Šì€ ì‚¬ìš©ì í•­ (Top-3 í›„ë³´ì™€ í•¨ê»˜ í‘œì‹œ)
            matched_user_ids = {m.matched_clause.id for m in result.match_results if m.is_matched}
            unmatched_results = [m for m in result.match_results if not m.is_matched and m.matched_clause is not None]
            
            # ì‚¬ìš©ì í•­ë³„ë¡œ ê·¸ë£¹í™”
            from collections import defaultdict
            unmatched_by_user = defaultdict(list)
            for match in unmatched_results:
                unmatched_by_user[match.matched_clause.id].append(match)
            
            if unmatched_by_user:
                f.write("\nâ“ ë§¤ì¹­ë˜ì§€ ì•Šì€ ì‚¬ìš©ì í•­ (ê´€ë ¨ ì¡°í•­ ë¶„ì„)\n")
                f.write("=" * 100 + "\n\n")
                
                for user_id, matches in unmatched_by_user.items():
                    # ì‚¬ìš©ì í•­ ì •ë³´
                    user_clause = matches[0].matched_clause
                    f.write(f"ğŸ“„ {user_clause.title}\n")
                    f.write(f"   {user_clause.text[:200]}...\n\n")
                    
                    f.write(f"   ğŸ’­ ê´€ë ¨ ì¡°í•­ ë¶„ì„ (Top {len(matches)}):\n")
                    f.write("   " + "-" * 90 + "\n\n")
                    
                    # Top-3 í›„ë³´ í‘œì‹œ
                    for idx, match in enumerate(matches[:3], 1):
                        f.write(f"   {idx}ï¸âƒ£ [{match.standard_clause.id}] {match.standard_clause.title}\n")
                        f.write(f"      ğŸ“Š ìœ ì‚¬ë„: {match.hybrid_score:.2f}\n")
                        if match.llm_decision:
                            f.write(f"      ğŸ¤– ì‹ ë¢°ë„: {match.llm_decision.confidence:.2f}\n")
                            if match.llm_decision.reasoning:
                                f.write(f"      ğŸ’­ LLM íŒë‹¨: {match.llm_decision.reasoning}\n")
                        f.write("\n")
                    
                    f.write("\n")
    
    def _run_verification(self, user_contract_path: str, report_format: str):
        """
        ê³„ì•½ì„œ ê²€ì¦ ì‹¤í–‰
        
        Args:
            user_contract_path: ì‚¬ìš©ì ê³„ì•½ì„œ íŒŒì¼ ê²½ë¡œ
            report_format: ë³´ê³ ì„œ í˜•ì‹ (text, pdf, both)
        """
        import sys
        from pathlib import Path
        
        # backend ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
        project_root = Path(__file__).parent.parent
        if str(project_root) not in sys.path:
            sys.path.insert(0, str(project_root))
        
        from backend.clause_verification.node_1_clause_matching.data_loader import ContractDataLoader
        from backend.clause_verification.node_1_clause_matching.verification_engine import ContractVerificationEngine
        from backend.clause_verification.node_1_clause_matching.embedding_service import EmbeddingService
        from backend.clause_verification.node_1_clause_matching.hybrid_search import HybridSearchEngine
        from backend.clause_verification.node_1_clause_matching.llm_verification import LLMVerificationService
        
        # í‘œì¤€ ê³„ì•½ì„œ ê²½ë¡œ (í•­ ë‹¨ìœ„ ì²­í‚¹ ë²„ì „)
        standard_contract_path = "data/chunked_documents/provide_std_contract_chunks.json"
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not Path(standard_contract_path).exists():
            logger.error(f"   [ERROR] í‘œì¤€ ê³„ì•½ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {standard_contract_path}")
            logger.error("   íŒíŠ¸: ë¨¼ì € 'python embed_std_contract_articles.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
            return
        
        if not Path(user_contract_path).exists():
            logger.error(f"   [ERROR] ì‚¬ìš©ì ê³„ì•½ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {user_contract_path}")
            return
        
        logger.info("\n=== 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ===")
        
        # ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
        loader = ContractDataLoader()
        
        # í‘œì¤€ ê³„ì•½ì„œ ë¡œë“œ
        standard_clauses = loader.load_standard_contract()
        logger.info(f"   [OK] í‘œì¤€ ê³„ì•½ì„œ ë¡œë“œ: {len(standard_clauses)}ê°œ ì¡°ë¬¸")
        
        # ì‚¬ìš©ì ê³„ì•½ì„œ ë¡œë“œ (í•­ ë‹¨ìœ„ ì²­í‚¹ ë°©ì‹)
        # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
        encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig']
        user_text = None
        for encoding in encodings:
            try:
                with open(user_contract_path, 'r', encoding=encoding) as f:
                    user_text = f.read()
                logger.info(f"   [OK] íŒŒì¼ ì¸ì½”ë”© ê°ì§€: {encoding}")
                break
            except UnicodeDecodeError:
                continue
        
        if user_text is None:
            logger.error(f"   [ERROR] íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ì¸ì½”ë”©: {', '.join(encodings)}")
            return
        
        user_clauses = loader.load_user_contract_chunked(user_text)
        logger.info(f"   [OK] ì‚¬ìš©ì ê³„ì•½ì„œ ë¡œë“œ: {len(user_clauses)}ê°œ ì²­í¬ (í•­ ë‹¨ìœ„)")
        
        logger.info("\n=== 2ë‹¨ê³„: ê²€ì¦ ì—”ì§„ ì´ˆê¸°í™” ===")
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        embedding_service = EmbeddingService()
        hybrid_search = HybridSearchEngine()  # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš© (BM25: 0.3, FAISS: 0.7)
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„ (ì—†ìœ¼ë©´ ìë™ ìƒì„±ë¨)
        try:
            hybrid_search.load_faiss_index()
            logger.info("   [OK] ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ")
        except FileNotFoundError:
            logger.info("   [INFO] FAISS ì¸ë±ìŠ¤ ì—†ìŒ (ê²€ì¦ ì‹œ ìë™ ìƒì„±ë¨)")
        
        llm_verification = LLMVerificationService()
        
        # ê²€ì¦ ì—”ì§„ ì´ˆê¸°í™”
        engine = ContractVerificationEngine(
            embedding_service=embedding_service,
            hybrid_search=hybrid_search,
            llm_verification=llm_verification
        )
        
        logger.info("   [OK] ê²€ì¦ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info("\n=== 3ë‹¨ê³„: ê³„ì•½ì„œ ê²€ì¦ ìˆ˜í–‰ ===")
        
        # ê²€ì¦ ìˆ˜í–‰ (ì—­ë°©í–¥ ê²€ì¦: ì‚¬ìš©ìâ†’í‘œì¤€)
        result = engine.verify_contract_reverse(
            standard_clauses=standard_clauses,
            user_clauses=user_clauses,
            top_k_candidates=10,
            top_k_titles=5,
            min_confidence=0.5
        )
        
        logger.info(f"   [OK] ê²€ì¦ ì™„ë£Œ")
        logger.info(f"        - í‘œì¤€ ì¡°ë¬¸ ìˆ˜: {result.total_standard_clauses}")
        logger.info(f"        - ì‚¬ìš©ì ì²­í¬ ìˆ˜: {result.total_user_clauses}")
        logger.info(f"        - ë§¤ì¹­ëœ ì²­í¬: {result.matched_clauses}")
        logger.info(f"        - ëˆ„ë½ëœ ì¡°ë¬¸: {result.missing_count}")
        logger.info(f"        - ê²€ì¦ ì™„ë£Œìœ¨: {result.verification_rate:.1f}%")
        
        logger.info("\n=== 4ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ê·¸ë£¹í™” ===")
        
        # ì¡°ë¬¸ë³„ ê·¸ë£¹í™”
        grouped_results = self._group_results_by_article(result.match_results)
        logger.info(f"   [OK] ì¡°ë¬¸ë³„ ê·¸ë£¹: {len(grouped_results)}ê°œ")
        
        logger.info("\n=== 5ë‹¨ê³„: ìƒì„¸ ë³´ê³ ì„œ ìƒì„± ===")
        
        # ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = Path(f"data/reports/chunked_verification_report_{timestamp}.txt")
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        self._generate_detailed_report(report_path, result, grouped_results, user_clauses)
        
        logger.info(f"   [OK] ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {report_path}")
        
        # ì½˜ì†” ìš”ì•½ ì¶œë ¥
        summary = result.get_summary()
        print("\n" + "="*80)
        print("ğŸ“‹ ê³„ì•½ì„œ ê²€ì¦ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        print(f"\nğŸ“Š í†µê³„:")
        print(f"   ì´ ì²­í¬ ìˆ˜: {len(user_clauses)}ê°œ")
        print(f"   ë§¤ì¹­ëœ ì²­í¬: {summary['matched_clauses']}ê°œ")
        print(f"   ë§¤ì¹­ë¥ : {result.matched_clauses/len(user_clauses)*100:.1f}%")
        print(f"   ì¡°ë¬¸ë³„ ê·¸ë£¹: {len(grouped_results)}ê°œ")
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {report_path}")
        print("="*80)
    
    def do_exit(self, arg):
        logger.info("ingestion ì¢…ë£Œ")
        return True
    
    def emptyline(self):
        pass
    
    def default(self, line):
        logger.error(f" ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ {line}")
        logger.info(" 'help'ë¥¼ ì…ë ¥í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´ í™•ì¸")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    cli = IngestionCLI()
    try:
        cli.cmdloop()
    except KeyboardInterrupt:
        logger.info("\n\ningestion ì¢…ë£Œ")
        sys.exit(0)


if __name__ == "__main__":
    main()
